---
title: "Using STAN for Adaptive Shrinkage"
author: "Kushal Kumar Dey"
date: "Monday, May 25, 2015"
output: html_document
---

## The Model

Suppose we have the estimates of some parameters $\hat{\beta} = \left( \hat{\beta}_1, \hat{\beta}_2, \cdots, \hat{\beta}_n \right)$ and standard error of $\hat{\beta}$ is $se(\hat{\beta}) = \left( s_1, s_2, \cdots, s_n \right)$. Using this we want to perform shrinkage on the parameters $\beta = c(\beta_1, \beta_2, \cdots, \beta_n)$. We use the model

$$  L(\hat{\beta},\beta,s) \propto \prod_{n=1}^{N} exp \left (-0.5* \frac{(\hat{\beta}_n - \beta_n)^2}{s^2_n} \right )$$

which is equivalent to the assumption 

$$ \hat{\beta}_n \sim N \left ( \beta_n, s^2_n \right )  $$

We assume that all the $\beta_n$ are all iid with the prior 

$$ \beta_n \sim  \sum_{k=1}^{K} \pi_k N(0, \sigma^2_k)  $$

We use a full Bayes approach where we assume the following 3 scenarios 

$$  \pi(\sigma_k) \propto \frac{1}{\sigma_k} \qquad (I)  \qquad \pi(\sigma_k) \sim HCauchy(0,1000| 0) \qquad (II) \qquad \pi(\sigma_k) \propto Unif(0,1000) \qquad (III)  $$

where $HCacuhy(0,1000 |0)$ represents the $Cauchy(0,1000)$ distribution left truncated at $0$. For the mixing proportions, we use the following priors. One prior is

$$  \pi \sim Dir (\frac{K}{N}, \frac{K}{N}, \cdots, \frac{K}{N}) \qquad \qquad (I) $$

The other prior is 

$$ \eta \sim N(\mu_c, \Sigma_c)  $$

$$  \pi_k = \frac{exp(\eta_{k})}{ \sum_{l=1}^{K} exp(\eta_{l})}   \qquad \qquad (II)$$

The priors for $\mu_c$ and $\Sigma_c$ is given by 

$$ \Sigma_c = D^{T} \Omega_c D $$

$$ D =diag(\sigma_{c,1}, \sigma_{c,2}, \cdots, \sigma_{c,K}) $$

$$ \sigma_{c,k} \sim HCauchy(0,100 |0)  $$

$$ \mu_c \sim N(0,1000) \hspace{1 cm} \Omega_c \sim det(\Omega_c)^{\alpha-1} $$


## The Simulation Scheme and Output

We used a very simple set up. We assumed a vector and corresponding standard error vector as follows 

```{r sim_setup}
suppressMessages(library(rstan))
suppressMessages(library(ashr))

betahat=c(0,1.03,0.05,0.67,2.06,3.11,2.22,1.19,1.43,1.2,0.03,10.54,8.66);
se=c(0.01,1,0.2,0.15,0.44,0.62,0.31,0.29,0.01,0.11,0.50,10.11,10.43);

```


The ash posterior means under the above set up using the Standard ash notation is given by 

```{r ash_setup}
#setwd("D:/Matthew_Stephens_Project/Others (Technical)/STAN")
#setwd("/stan_files");
fit=ash(betahat,se,mixcompdist="normal")
fitted_post_mean=fit$PosteriorMean;
```

Above we used the standard noation of "ash". Now for our models, we assumed the following set up with number of clusters $K=5$.
```{r stan-ash_setup}
K=5;
N=13;
dir_param=rep(K/N,K);

test_data.ash <- list(N=13,
                      K=5,
                      betahat=betahat,
                      se=se,
                      alpha=dir_param);

```

We pulled the results from full Bayes ash methods using STAN for 6 different set ups (combination of the 3 priors for $\sigma$ and the 2 priors for the mixing proportions $\pi$ mentioned above) and we present the posterior means of $\beta$ for each of the methods next.We use the following notation for the corresponding models

- $ash.standard \; \;$ :      the Standard ash model as in the ash(.) function in "ashr" package
- $ash.jeffrey.dir \; \;$:    prior $\sigma_k$ Jeffrey's prior and mixing prop is $Dir(K/N,K/N,\cdots,K/N)$.
- $ash.jeffrey.norm \; \;$:    prior $\sigma_k$ Jeffrey's prior and mixing prop is $N(\mu_c, \Sigma_c)$ transformed.
- $ash.hcauchy.dir \; \;$:    prior $\sigma_k$ HCauchy(0|0,1000) prior and mixing prop is $Dir(K/N,K/N,\cdots,K/N)$.
- $ash.hcauchy.norm \; \;$:    prior $\sigma_k$ HCauchy(0|0,1000) prior and mixing prop is $N(\mu_c, \Sigma_c)$ transformed.
- $ash.unif.dir \; \;$:    prior $\sigma_k$ Unif(0,1000) prior and mixing prop is $Dir(K/N,K/N,\cdots,K/N)$.
- $ash.unif.norm \; \;$:    prior $\sigma_k$ Unif(0|0,1000) prior and mixing prop is $N(\mu_c, \Sigma_c)$ transformed.

The table of the actual vectors of $\hat{\beta}$, $se$ and the results of the transformation under $K=5$ and the configuration mentioned above

```{r, echo=FALSE, eval=TRUE, cache=TRUE}
K=5;
N=13;
dir_param=rep(K/N,K);

test_data.ash <- list(N=13,
                      K=5,
                      betahat=betahat,
                      se=se,
                      alpha=dir_param);

betahat=c(0,1.03,0.05,0.67,2.06,3.11,2.22,1.19,1.43,1.2,0.03,10.54,8.66);
se=c(0.01,1,0.2,0.15,0.44,0.62,0.31,0.29,0.01,0.11,0.50,10.11,10.43);
vec1 <- fitted_post_mean;
vec2 <- c(0.001,0.760,0.067,0.652,1.915,2.376,2.125,1.113,1.430,1.192,0.054,0.217,-0.196)
vec3 <- c(-0.001,0.667,0.053,0.658,1.961,2.664,2.090,1.141,1.431,1.202,0.007,0.310,0.344);
vec4 <- c(0.000, 0.725, 0.051, 0.664, 1.894, 2.662, 2.128, 1.148, 1.430, 1.193, 0.025, 0.401, 0.298)
vec5 <- c(0.000, 0.729, 0.049, 0.659, 1.900, 2.657, 2.137, 1.154, 1.430, 1.192, 0.025, 0.299, 0.216)
vec6 <- c(0.001, 0.761, 0.047, 0.659, 1.894, 2.623, 2.117, 1.141, 1.431, 1.208, 0.058, 0.264, -0.113)
vec7 <- c(0.000, 0.607, 0.047, 0.655, 1.876, 2.648, 2.138, 1.144, 1.430, 1.187, 0.039, 0.229, 0.233)

data.frame=rbind(betahat,se,vec1,vec2,vec3,vec4,vec5,vec6,vec7);
rownames(data.frame)=c("betahat","se","ash.standard","ash.jeffrey.dir","ash.jeffrey.norm",
                       "ash.hcauchy.dir","ash.hcauchy.norm","ash.unif.dir","ash.unif.norm");
print(data.frame)
```

We see some difference between the results of the full Bayes ash approaches used by me and the standard ash approach when the variance is relatively high compared to the mean. For cases where it is not, there seems to be close correspondence between the standard ash and these methods.

## Discussion

I checked that STAN codes are not amenable to the use of the usual profiling tool (lineprof). 

The STAN codes and the .RMd file are uploaded on "github/kkdey".

There are a few issues that need to be addressed

- The documentation in Git repo "ashr" states that a $t$ log likelihood is used with appropriate degrees of freedom $\nu$. How is the $\nu$ selected?

- How many components in the mixture of normals, $K$, are taken to optimize the shrinkage? I may be wrong, but it seemed too many classes could reduce the amount of shrinkage for the large values may be?

- The Git repo mentions Empirical Bayes methods being used to do inference. Does that mean the use of basic maximum log likelihood estimation? Would be nice to know that in order to compare the different methods. STAN mainly uses a Hamiltonian Monte Carlo scheme for making inference

- STAN-ash methods are usually slower than the standard ash methods, but from my previous experience with Monte Carlo samplers, I must admit that it is actually a pretty fast Monte Carlo sampling scheme.

- Finally, how to generalize this scheme for the Generalized Linear models scheme you discussed?


In the following section, I present the codes used to fit each of the 6 models.

## Supplementary:  STAN codes 

The Stan-ash scheme under dirichlet distribution for the mixing proportions and the Jeffrey's prior for $\sigma$ is given by

```{r, echo=TRUE, eval=FALSE, cache=TRUE}
K=5;
N=13;
dir_param=rep(K/N,K);

test_data.ash <- list(N=13,
                      K=5,
                      betahat=betahat,
                      se=se,
                      alpha=dir_param);

cat <- function(...) return(invisible(NULL))
Sys.setenv(nodosfilewarning = "1")
# system("sed -i '/-Wunused-function/d' a")
# system("sed -i '/Error in function/d' a")

model_code <- "data {
          int<lower=1> N;
          int<lower=1> K;
          vector[N] betahat;
          vector<lower=0>[N] se;
          vector<lower=0>[K] alpha;
       }

  parameters {
                vector<lower=0>[K] sigma;
                simplex[K] prop;
                vector[N] beta ;
             }

  transformed parameters {
                            vector[K] log_sigma;
                            log_sigma <- log(sigma);
                         }

  model {
    
            real ps[K];
            prop ~ dirichlet(alpha);

            for(n in 1:N)
            {
                //....  Mixture normal prior for beta 
                for(k in 1:K)
                {
                    ps[k] <- log(prop[k])+ normal_log(beta[n],0,exp(log_sigma[k])); 
                }
                increment_log_prob(log_sum_exp(ps));
                betahat[n] ~ normal(beta[n],se[n]); //...  the data likelihood
            }
        }
"

fit1 <- suppressMessages(suppressWarnings(stan(file="ash_reg_logsigma_propdir.stan", data=test_data.ash,iter=10000,chains=4,verbose=FALSE)));

temp=summary(fit1)$summary[,"mean"]
beta_temp.1=as.numeric(temp[grep("beta",names(temp))]);
round(beta_temp.1,digits=3)

round(fitted_post_mean,digits=3)
```


The Stan-ash scheme under Normal-transformed  distribution for the mixing proportions and the Jeffrey's prior for $\sigma$ is given by

```{r, echo=TRUE, eval=FALSE, cache=TRUE}

K=5;
N=13;
dir_param=rep(K/N,K);

test_data.ash <- list(N=13,
                      K=5,
                      betahat=betahat,
                      se=se);

model_code <- "data {
          int<lower=1> N;
          int<lower=1> K;
          vector[N] betahat;
          vector<lower=0>[N] se;

       }

parameters {
              vector<lower=0>[K] sigma; //... scales of individual mixing components
              vector[K] eta;  //... mixing proportions un-transformed
              vector[N] beta ; //... the true mean
              vector[K] mu_prop; //... the means for CTM prop
              vector<lower=0>[K] sigma_prop_vec; //... the scales for CTM prop
              corr_matrix[K] Omega_prop;  //... correlation matrix
           }

transformed parameters {
                            vector[K] log_sigma;
                            cov_matrix[K] Sigma_prop;
                            simplex[K] prop;
                            
                            log_sigma <- log(sigma);
                            Sigma_prop <- diag_matrix(sigma_prop_vec) * Omega_prop * diag_matrix(sigma_prop_vec);
                            prop <- softmax(eta);

                       }

model {
        real ps[K];
        mu_prop ~ normal(0,100); // vectorized, diffuse
        Omega_prop ~ lkj_corr(3.0); // regularize to unit correlation
        sigma_prop_vec ~ cauchy(0,100); // half-Cauchy due to constraint
        
        eta ~ multi_normal(mu_prop,Sigma_prop);
        
        for(n in 1:N)
        {
            //....  Mixture normal prior for beta 
            for(k in 1:K)
            {
                ps[k] <- log(prop[k])+ normal_log(beta[n],0,exp(log_sigma[k])); 
            }
            increment_log_prob(log_sum_exp(ps));
            betahat[n] ~ normal(beta[n],se[n]); //...  the data likelihood
         }
      }
"


fit1 <- suppressMessages(suppressWarnings(stan(file="ash_reg_logsigma_propctm.stan", data=test_data.ash,iter=10000,chains=2, verbose=FALSE)));

temp=summary(fit1)$summary[,"mean"]
beta_temp.2=as.numeric(temp[grep("beta",names(temp))]);
round(beta_temp.2,digits=3)

round(fitted_post_mean,digits=3)

```



The Stan-ash scheme under dirichlet distribution for the mixing proportions and the Half Cauchy (0 | 0,1000) prior for $\sigma$ is given by

```{r, echo=TRUE, eval=FALSE, cache=TRUE}
K=5;
N=13;
dir_param=rep(K/N,K);

test_data.ash <- list(N=13,
                      K=5,
                      betahat=betahat,
                      se=se,
                      alpha=dir_param);


model_code <-" data {
          int<lower=1> N;
          int<lower=1> K;
          vector[N] betahat;
          vector<lower=0>[N] se;
          vector<lower=0>[K] alpha;
       }

  parameters {
                vector<lower=0>[K] sigma;
                simplex[K] prop;
                vector[N] beta ;
             }

    model {
             real ps[K];
             prop ~ dirichlet(alpha); //...  prior for mixing proportions
             sigma ~ cauchy(0,1000);  //.... prior for the variation
             for(n in 1:N)
             {
                  //....  Mixture normal prior for beta 
                  for(k in 1:K)
                  {
                      ps[k] <- log(prop[k])+ normal_log(beta[n],0,sigma[k]); 
                  }
                  increment_log_prob(log_sum_exp(ps));
                  betahat[n] ~ normal(beta[n],se[n]); //...  the data likelihood
             }
           }
  "


fit1 <- suppressMessages(suppressWarnings(stan(file="ash_reg_hcauchysigma_propdir.stan", data=test_data.ash,iter=10000,chains=4)));

temp=summary(fit1)$summary[,"mean"]
beta_temp.3=as.numeric(temp[grep("beta",names(temp))]);
round(beta_temp.3,digits=3)

round(fitted_post_mean,digits=3)
```


The Stan-ash scheme under Normal-transformed  distribution for the mixing proportions and the Half Cauchy(0 | 0,1000) prior for $\sigma$ is given by

```{r, echo=TRUE, eval=FALSE, cache=TRUE}

K=5;
N=13;
dir_param=rep(K/N,K);

test_data.ash <- list(N=13,
                      K=5,
                      betahat=betahat,
                      se=se);

model_code <- "data {
          int<lower=1> N;
          int<lower=1> K;
          vector[N] betahat;
          vector<lower=0>[N] se;
       }

parameters {
              vector<lower=0>[K] sigma; //... scales of individual mixing components
              vector[K] eta;  //... mixing proportions un-transformed
              vector[N] beta ; //... the true mean
              vector[K] mu_prop; //... the means for CTM prop
              vector<lower=0>[K] sigma_prop_vec; //... the scales for CTM prop
              corr_matrix[K] Omega_prop;  //... correlation matrix
           }

transformed parameters {
                          cov_matrix[K] Sigma_prop;
                          simplex[K] prop;
                          Sigma_prop <- diag_matrix(sigma_prop_vec) * Omega_prop * diag_matrix(sigma_prop_vec);
                          prop <- softmax(eta);
  
                       }

model {
          real ps[K];
          mu_prop ~ normal(0,100); // vectorized, diffuse
          Omega_prop ~ lkj_corr(3.0); // regularize to unit correlation
          sigma_prop_vec ~ cauchy(0,100); // half-Cauchy due to constraint
          sigma ~ cauchy(0,1000);  //.... prior for the variation
          
          eta ~ multi_normal(mu_prop,Sigma_prop);
  
          for(n in 1:N)
          {
              //....  Mixture normal prior for beta 
              for(k in 1:K)
              {
                  ps[k] <- log(prop[k])+ normal_log(beta[n],0,sigma[k]); 
              }
              increment_log_prob(log_sum_exp(ps));
              betahat[n] ~ normal(beta[n],se[n]); //...  the data likelihood
          }
      }
"

fit1 <- suppressMessages(suppressWarnings(stan(file="ash_reg_hcauchysigma_propctm.stan", data=test_data.ash,iter=10000,chains=2)));

temp=summary(fit1)$summary[,"mean"]
beta_temp.4=as.numeric(temp[grep("beta",names(temp))]);
round(beta_temp.4,digits=3)

round(fitted_post_mean,digits=3)

```


The Stan-ash scheme under dirichlet distribution for the mixing proportions and the uniform(0,1000) prior for $\sigma$ is given by

```{r, echo=TRUE, eval=FALSE, cache=TRUE}
K=5;
N=13;
dir_param=rep(K/N,K);

test_data.ash <- list(N=13,
                      K=5,
                      betahat=betahat,
                      se=se,
                      alpha=dir_param);
model_code <- " data {
          int<lower=1> N;
          int<lower=1> K;
          vector[N] betahat;
          vector<lower=0>[N] se;
       }

parameters {
              vector<lower=0>[K] sigma;
              simplex[K] prop;
              vector[N] beta ;
           }

model {
          real ps[K];
          sigma ~ uniform(0,1000);  //.... prior for the variation

          for(n in 1:N)
          {
              //....  Mixture normal prior for beta 
              for(k in 1:K)
              {
                  ps[k] <- log(prop[k])+ normal_log(beta[n],0,sigma[k]); 
              }
              increment_log_prob(log_sum_exp(ps));
              betahat[n] ~ normal(beta[n],se[n]); //...  the data likelihood
          }
      }
"

fit1 <- suppressMessages(suppressWarnings(stan(file="ash_reg_unifsigma_propdir.stan", data=test_data.ash,iter=10000,chains=4)));
temp=summary(fit1)$summary[,"mean"]
beta_temp.5=as.numeric(temp[grep("beta",names(temp))]);
round(beta_temp.5,digits=3)

round(fitted_post_mean,digits=3)
```


The Stan-ash scheme under Normal-transformed  distribution for the mixing proportions and uniform $(0, 1000)$ prior for $\sigma$ is given by

```{r, echo=TRUE, eval=FALSE, cache=TRUE}

K=5;
N=13;
dir_param=rep(K/N,K);

test_data.ash <- list(N=13,
                      K=5,
                      betahat=betahat,
                      se=se);

model_code <- "  data {
          int<lower=1> N;
          int<lower=1> K;
          vector[N] betahat;
          vector<lower=0>[N] se;
       }

parameters {
              vector<lower=0>[K] sigma; //... scales of individual mixing components
              vector[K] eta;  //... mixing proportions un-transformed
              vector[N] beta ; //... the true mean
              vector[K] mu_prop; //... the means for CTM prop
              vector<lower=0>[K] sigma_prop_vec; //... the scales for CTM prop
              corr_matrix[K] Omega_prop;  //... correlation matrix
           }

transformed parameters {
                          cov_matrix[K] Sigma_prop;
                          simplex[K] prop;
                          Sigma_prop <- diag_matrix(sigma_prop_vec) * Omega_prop * diag_matrix(sigma_prop_vec);
                          prop <- softmax(eta);
  
                       }

model {
          real ps[K];
          mu_prop ~ normal(0,100); // vectorized, diffuse
          Omega_prop ~ lkj_corr(3.0); // regularize to unit correlation
          sigma_prop_vec ~ cauchy(0,100); // half-Cauchy due to constraint
          sigma ~ uniform(0,1000);  //.... prior for the variation

  
          eta ~ multi_normal(mu_prop,Sigma_prop);
  
          for(n in 1:N)
          {
              //....  Mixture normal prior for beta 
              for(k in 1:K)
              {
                ps[k] <- log(prop[k])+ normal_log(beta[n],0,sigma[k]); 
              }
              increment_log_prob(log_sum_exp(ps));
              betahat[n] ~ normal(beta[n],se[n]); //...  the data likelihood
           }
      }
"
  
fit1 <- suppressMessages(suppressWarnings(stan(file="ash_reg_unifsigma_propctm.stan", data=test_data.ash,iter=10000,chains=2)));

temp=summary(fit1)$summary[,"mean"]
beta_temp.6=as.numeric(temp[grep("beta",names(temp))]);
round(beta_temp.6,digits=3)

round(fitted_post_mean,digits=3)

```




